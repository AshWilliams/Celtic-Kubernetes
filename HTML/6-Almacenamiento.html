<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Almacenamiento para Kubernetes</title>
  <style type="text/css">@import "css/style.css";</style><link rel="alternate stylesheet" type="text/css" href="resource://gre-resources/plaintext.css" title="Ajustar líneas largas"><link href="css/sss.css" type="text/css" rel="stylesheet"><link href="css/sss.print.css" type="text/css" media="print" rel="stylesheet"><link href="css/default.css" type="text/css" rel="stylesheet"><meta content="width=device-width, initial-scale=1" name="viewport"><link rel="icon" type="image/x-icon" href="Imagenes/celtic-kubernetes.ico">
</head>
<body>
<div id="header">
<ul>
  <li>
<a class="bar" href="1-Portada.html">Home</a>
</li>
  <li>
<a class="bar" href="2-Kube_simple.html">Kubernetes Simple</a>
</li>
  <li>
<a class="bar" href="3-Kube_HA_pcs.html">Kubernetes HA</a>
</li>
  <li>
<a class="bar" href="4-Addons.html">Addons</a>
</li>
  <li>
<a class="bar" href="5-Exponer_svc.html">Exponer servicios</a>
</li>
  <li>
<a class="active" href="6-Almacenamiento.html">Almacenamiento persistente</a>
</li>
  <li>
<a class="bar" href="7-Explotando_kubernetes.html">Utilización</a>
</li>
  <li>
<a class="bar" href="8-Kubernetes_ansible.html">Kubernetes y Ansible</a>
</li>
  <li>
<a class="bar" href="9-ElasticKube.html">ElasticKube</a>
</li>
  <li>
<a class="bar" href="10-Conclusion.html">Conclusión</a>
</li>
  <li style="float:bottom">
<a class="bar" href="Contacto.html">Contacto</a>
</li>
</ul>
</div>
<div id="control">
<ul>
  <li>
<a class="next" href="5-Exponer_svc.html">Anterior</a>
</li>
  <li style="float:right">
<a class="next" href="7-Explotando_kubernetes.html">Siguiente</a>
</li>
</ul>
</div>
<h1 id="almacenamiento-para-kubernetes">Almacenamiento para Kubernetes</h1>
<ul>
<li><a href="http://severalnines.com/blog/wordpress-application-clustering-using-kubernetes-haproxy-and-keepalived">Fuente severalnines</a></li>
</ul>
<p>Para dar almacenamiento persistente en Kubernetes, podrmos utilizar:</p>
<ul>
<li>NFS</li>
<li>iSCSI</li>
<li>RBD (Ceph Block Device)</li>
<li>Glusterfs</li>
<li>HostPath</li>
<li>GCEPersistentDisk</li>
<li>AWSElasticBlockStore</li>
</ul>
<p>Para cualquiera de ellas en Kubernetes tendremos que crear:</p>
<ul>
<li>PersistentVolume -- Donde especificamos el volumen persistente</li>
<li>PersistentVolumeClaim -- Donde reclamamos espacio en el volumen</li>
</ul>
<p>Modos de acceso:</p>
<ul>
<li>ReadWriteOnce -- read-write solo para un nodo (RWO)</li>
<li>ReadOnlyMany -- read-only para muchos nodos (ROX)</li>
<li>ReadWriteMany -- read-write para muchos nodos (RWX)</li>
</ul>
<p>Políticas de reciclaje de volúmenes son son:</p>
<ul>
<li>Retain - Reclamación manual</li>
<li>Recycle - Reutilizar contenido</li>
<li>Delete - Borrar contenido</li>
</ul>
<p>Estados de un volumen:</p>
<ul>
<li>Available - disponible para reclamación</li>
<li>Bound - No disponible, se esta utilizando por una reclamación.</li>
<li>Released - La reclamación del volumen se a eliminado y esta esperando otra petición del cluster.</li>
<li>Failed - En estado de fallo.</li>
</ul>
<h2 id="servidor-nfs-angus">Servidor NFS Angus</h2>
<p>En el servidor Angus, procedemos a montar el almacenamiento</p>
<pre><code>yum install nfs-utils</code></pre>
<p>El directorio que compartiremos sera <code>/shared/kubernetes</code></p>
<p>Creamos dos directorios dentro para el almacenamiento de los sitios web y para las bases de datos</p>
<pre><code>mkdir -p /shared/kubernetes/www
mkdir -p /shared/kubernetes/db</code></pre>
<p>Añadimos la siguiente linea para dar acceso a la red privada de OpenStack del resto de maquinas</p>
<p><code>/etc/exports</code></p>
<pre><code>/shared 10.0.0.0/24(rw,sync,no_root_squash,no_all_squash)</code></pre>
<p>Habilitamos y arrancamos las siguientes unidades para dar el servicio de NFS</p>
<pre><code>systemctl enable rpcbind; systemctl enable nfs-server; systemctl restart rpcbind; systemctl restart nfs-server</code></pre>
<h4 id="clientes-nfs-artio-y-esus">Clientes NFS Artio y Esus</h4>
<p>Instamalos el cliente NFS en los minions</p>
<pre><code>yum install nfs-utils</code></pre>
<p>Comprobamos que la configuración anterior en Angus funciona</p>
<pre><code>[root@artio centos]# showmount -e 10.0.0.52
Export list for 10.0.0.52:
/shared 10.0.0.0/24</code></pre>
<p>Podemos montarlo con</p>
<pre><code>mount -t nfs4 10.0.0.52:/shared /tu_directorio</code></pre>
<h4 id="creando-almacenamiento-persistente-con-nfs">Creando almacenamiento persistente con NFS</h4>
<p>En uno de los nodos master del cluster de Kubernetes creamos el siguientes ficheros</p>
<p>Creamos el volumen persistente para los sitios web <code>kubectl create -f nfs-www.yaml</code></p>
<pre><code>apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-5g-www
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: /shared/kubernetes/www
    server: 10.0.0.52</code></pre>
<p>Creamos el volumen persistente para la base de datos <code>kubectl create -f nfs-db.yaml</code></p>
<pre><code>apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-5g-db
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: /shared/kubernetes/db
    server: 10.0.0.52</code></pre>
<ul>
<li>Si queremos borrarlo solo tendremos que ejecutar <code>kubectl delete pv pv-5g-www</code></li>
</ul>
<p>Podemos ver el estado con</p>
<pre><code>[root@morrigan config-kubernetes]# kubectl get pv
NAME        LABELS    CAPACITY   ACCESSMODES   STATUS      CLAIM     REASON    AGE
pv-3g-www   &lt;none&gt;    3Gi        RWX           Available                       1m
pv-5g-db    &lt;none&gt;    5Gi        RWX           Available                       1m</code></pre>
<h4 id="reclamando-almacenamiento">Reclamando almacenamiento</h4>
<p>En uno de los nodos master del cluster de Kubernetes realizamos claim de 1Gi para nuestro servicio web</p>
<p><code>kubectl create -f claim1-www.yaml</code></p>
<pre><code>kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: myclaim1-www
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi</code></pre>
<p>También creamos una reclamación de almacenamiento para db</p>
<p><code>kubectl create -f claim1-db.yaml</code></p>
<pre><code>kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: myclaim1-db
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 2Gi</code></pre>
<p>Podemos consultar el estado de la siguiente forma</p>
<pre><code>[root@morrigan config-kubernetes]# kubectl get pv,pvc
NAME           LABELS    CAPACITY   ACCESSMODES   STATUS     CLAIM                  REASON    AGE
pv-5g-db       &lt;none&gt;    5Gi        RWX           Bound      default/myclaim1-db              12m
pv-5g-www      &lt;none&gt;    5Gi        RWX           Bound      default/myclaim1-www             5m
NAME           LABELS    STATUS     VOLUME        CAPACITY   ACCESSMODES            AGE
myclaim1-db    &lt;none&gt;    Bound      pv-5g-db      5Gi        RWX                    20s
myclaim1-www   &lt;none&gt;    Bound      pv-5g-www     5Gi        RWX                    3m</code></pre>
<div id='glusterfs'/>

<h2 id="glusterfs-en-angus-y-dagda">GlusterFS en Angus y Dagda</h2>
<p>Una vez funcionando NFS tendremos un unico punto de fallo, para solucionar esto montaremos GlusterFS para proporcionar HA.</p>
<ul>
<li><a href="https://wiki.centos.org/SpecialInterestGroup/Storage/gluster-Quickstart">Fuente CentOS WIKI</a></li>
<li><a href="http://www.unixmen.com/install-glusterfs-server-client-centos-7/">Fuente unixmen</a></li>
<li><a href="https://www.digitalocean.com/community/tutorials/how-to-create-a-redundant-storage-pool-using-glusterfs-on-ubuntu-servers">Fuente digitalocean</a></li>
</ul>
<p>Eliminamos el servicio NFS y todos sus ficheros para tener un escenario limpio</p>
<pre><code>yum remove nfs-utils</code></pre>
<p>Instalamos GLusterFS en Angus y Dagna</p>
<pre><code>yum -y install glusterfs glusterfs-fuse glusterfs-server</code></pre>
<p>En mi caso, asociare un volumen de 10G a cada una de las maquinas virtuales</p>
<p>Listamos los volúmenes</p>
<p><code>lsblk</code></p>
<pre><code>NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
vda    253:0    0   8G  0 disk 
└─vda1 253:1    0   8G  0 part /
vdb    253:16   0  10G  0 disk </code></pre>
<p>Damos formato</p>
<pre><code>mkfs.xfs /dev/vdb</code></pre>
<p>Creamos el directorio donde montaremos el volumen</p>
<pre><code>mkdir -p /bricks/brick1</code></pre>
<p>Montamos el volumen y le damos formato:</p>
<pre><code>mount /dev/vdb /bricks/brick1</code></pre>
<p>Comprobamos el estado del volumen</p>
<p><code>df -h</code></p>
<pre><code>S.ficheros     Tamaño Usados  Disp Uso% Montado en
/dev/vda1        8,0G   1,5G  6,6G  19% /
devtmpfs         227M      0  227M   0% /dev
tmpfs            245M      0  245M   0% /dev/shm
tmpfs            245M   8,3M  237M   4% /run
tmpfs            245M      0  245M   0% /sys/fs/cgroup
tmpfs             49M      0   49M   0% /run/user/1000
/dev/vdb          10G    33M   10G   1% /bricks/brick1</code></pre>
<p>Por ultimo ara que se monte automáticamente lo añadimos a /etc/fstab</p>
<p><code>nano /etc/fstab</code></p>
<pre><code>/dev/vdb    /bricks/brick1  xfs     defaults    1   2</code></pre>
<p>Añadimos Angus y Dagda</p>
<ul>
<li><p>Angus</p>
<pre><code>[root@angus centos]# gluster peer probe dagda
peer probe: success. 
[root@angus centos]# gluster peer status
Number of Peers: 1

Hostname: dagda
Uuid: 3fd499e0-75c7-4552-b8c0-5b77851288d3
State: Peer in Cluster (Connected)
[root@angus centos]# </code></pre></li>
<li><p>Dagda</p>
<pre><code>[root@dagda centos]# gluster peer probe angus
peer probe: success. Host angus port 24007 already in peer list
[root@dagda centos]# gluster peer status
Number of Peers: 1

Hostname: angus
Uuid: 27cbd79a-e8fa-4102-a6e3-d7e5e3d770d9
State: Peer in Cluster (Connected)</code></pre></li>
</ul>
<p>Vemos el estado de pool</p>
<pre><code>[root@angus centos]# gluster pool list
UUID                    Hostname    State
3fd499e0-75c7-4552-b8c0-5b77851288d3    dagda       Connected 
27cbd79a-e8fa-4102-a6e3-d7e5e3d770d9    localhost   Connected

[root@angus centos]# gluster volume status
No volumes present</code></pre>
<p>Creamos el volumen distribuido</p>
<pre><code>gluster volume create dist-volume angus:/bricks/brick1 dagda:/bricks/brick1 force

gluster volume create dist-volume replica 2 angus:/bricks/brick1 dagda:/bricks/brick1 force</code></pre>
<p>Iniciamos el volumen distribuido</p>
<pre><code>gluster volume start dist-volume</code></pre>
<p>Con esto ya tendremos nuestro servidor listo, podemos ver la información de los volúmenes</p>
<pre><code>[root@angus centos]# gluster volume info
Volume Name: dist-volume
Type: Distribute
Volume ID: a271e529-023a-4be8-89be-e17108979b2c
Status: Started
Number of Bricks: 2
Transport-type: tcp
Bricks:
Brick1: angus:/bricks/brick1
Brick2: dagda:/bricks/brick1
Options Reconfigured:
performance.readdir-ahead: on

[root@angus centos]# gluster volume status
Status of volume: dist-volume
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick angus:/bricks/brick1                  49152     0          Y       9409 
Brick dagda:/bricks/brick1                  49152     0          Y       9218 
NFS Server on localhost                     2049      0          Y       9429 
NFS Server on dagda                         2049      0          Y       9238 
 
Task Status of Volume dist-volume
------------------------------------------------------------------------------
There are no active volume tasks</code></pre>
<h4 id="clientes-de-glusterfs-artio-y-eusus">Clientes de GlusterFS (Artio y Eusus)</h4>
<p>Instalamos el siguiente software</p>
<pre><code>yum -y install glusterfs glusterfs-fuse</code></pre>
<p>Creamos el directorio donde añadiremos el volumen distribuido</p>
<pre><code>mkdir  /mnt/gluster-v-dist</code></pre>
<p>Montamos el volumen distribuido</p>
<pre><code>mount.glusterfs  angus:/dist-volume   /mnt/gluster-v-dist/</code></pre>
<p>Podemos ver el resultado con</p>
<pre><code>[root@artio centos]# df  -h 
S.ficheros         Tamaño Usados  Disp Uso% Montado en
/dev/vda1             20G    13G  8,0G  61% /
devtmpfs             902M      0  902M   0% /dev
tmpfs                920M      0  920M   0% /dev/shm
tmpfs                920M    17M  904M   2% /run
tmpfs                920M      0  920M   0% /sys/fs/cgroup
tmpfs                920M   4,0K  920M   1% /var/lib/kubelet/pods/8fe$
tmpfs                184M      0  184M   0% /run/user/1000
tmpfs                184M      0  184M   0% /run/user/0
angus:/dist-volume    20G    65M   20G   1% /mnt/gluster-v-dist</code></pre>
<p>Para que se monte automáticamente añadimos la sigueinte linea a <code>/etc/fstab</code></p>
<pre><code>angus:/dist-volume   /mnt/gluster-v-dist  glusterfs defaults,_netdev 0 0</code></pre>
<h4 id="test-de-funcionamiento-de-glusterfs">Test de funcionamiento de GlusterFS</h4>
<p>Creamos dos ficheros de ejemplo en Artio</p>
<pre><code>touch /mnt/gluster-v-dist/file1
touch /mnt/gluster-v-dist/file2</code></pre>
<p>Paramos Angus</p>
<pre><code>halt</code></pre>
<p>Borramos un fichero en Artio</p>
<pre><code>rm -rf /mnt/gluster-v-dist/file2</code></pre>
<p>Listamos el directorio en Esus</p>
<pre><code>[root@esus centos]# ls /mnt/gluster-v-dist
file1</code></pre>
<p>Como podemos ver el cambio esta sincronizado gracias a Dagda ya que Angus esta parado.</p>
<h4 id="creando-almacenamiento-persistente-con-glusterfs">Creando almacenamiento persistente con GlusterFS</h4>
<p>En uno de los nodos master del cluster de Kubernetes creamos los siguientes ficheros</p>
<p>Lo primero que tenemos que hacer es crear un endpoint de nuestro nodos de GlusterFS con el puerto que queramos dentro del rango que acepta Kubernetes y sin namespace.</p>
<p><code>kubectl create -f glusterfs-endpoint.yaml</code></p>
<pre><code>kind: Endpoints
apiVersion: v1
metadata: 
  labels:
    name: glusterfs-cluster
  name: glusterfs-cluster
subsets: 
  - addresses: 
      - ip: 10.0.0.52
    ports:
      - port: 1
  - addresses:   
      - ip: 10.0.0.50
    ports:
      - port: 1</code></pre>
<p>Este endpoint puede dar problemas de persistencia * <a href="https://github.com/kubernetes/kubernetes/issues/12964">Endpoints are not persistented #12964</a> * <a href="https://github.com/kubernetes/kubernetes/issues/13511">Glusterfs volumes are unstable #13511</a></p>
<p>Para consegir tener nuestro endpoint persistente creamos el siguiente pod</p>
<pre><code>{
    &quot;apiVersion&quot;: &quot;v1&quot;,
    &quot;kind&quot;: &quot;Pod&quot;,
    &quot;metadata&quot;: {
        &quot;name&quot;: &quot;glusterfs&quot;
    },
    &quot;spec&quot;: {
        &quot;containers&quot;: [
            {
                &quot;name&quot;: &quot;glusterfs&quot;,
                &quot;image&quot;: &quot;kubernetes/pause&quot;,
                &quot;volumeMounts&quot;: [
                    {
                        &quot;mountPath&quot;: &quot;/mnt/glusterfs-v-dist&quot;,
                        &quot;name&quot;: &quot;glusterfsvol&quot;
                    }
                ]
            }
        ],
        &quot;volumes&quot;: [
            {
                &quot;name&quot;: &quot;glusterfsvol&quot;,
                &quot;glusterfs&quot;: {
                    &quot;endpoints&quot;: &quot;glusterfs-cluster&quot;,
                    &quot;path&quot;: &quot;dist-volume&quot;,
                    &quot;readOnly&quot;: true
                }
            }
        ]
    }
}</code></pre>
<p>Podemos ver el estado de los endpoint con</p>
<pre><code>[root@morrigan config-kubernetes]# kubectl get endpoints | grep glusterfs-cluster
glusterfs-cluster   10.0.0.50:1,10.0.0.52:1       38s</code></pre>
<p>Creamos un servicio para el endpoint anterior</p>
<p><code>kubectl create -f glusterfs-service.yaml</code></p>
<pre><code>apiVersion: &quot;v1&quot;
kind: &quot;Service&quot;
metadata:
  labels:
    name: glusterfs-cluster
  name: &quot;glusterfs-cluster&quot;
spec:
  ports:
  - port: 1</code></pre>
<p>Con esto tendremos nuestro endpoint persistente</p>
<p>Creamos el volumen persistente de ejemplo para una base de datos <code>kubectl create -f glusterfs-db.yaml</code></p>
<pre><code>apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-5g-db-1
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteMany
  glusterfs:
    path: dist-volume
    endpoints: glusterfs-cluster
    readOnly: false
  persistentVolumeReclaimPolicy: Recycle</code></pre>
<p>Creamos una reserva de ejemplo de 2Gi</p>
<p><code>kubectl create -f claim-1-db.yaml</code></p>
<pre><code>kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: myclaim-1-db
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 2Gi</code></pre>
<hr />
<div id="control">
<ul>
  <li>
<a class="next" href="5-Exponer_svc.html">Anterior</a>
</li>
  <li style="float:right">
<a class="next" href="7-Explotando_kubernetes.html">Siguiente</a>
</li>
</ul>
</div>
</body>
</html>
